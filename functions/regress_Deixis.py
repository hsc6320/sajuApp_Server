# -*- coding: utf-8 -*-
"""
ëŒ€í™” íšŒê·€ + ì§€ì‹œì–´(ì´ë•Œ/ê±°ê¸°/ê·¸ ì‚¬ëŒ...) ì•µì»¤ë§ í†µí•© ë¹Œë” (V2)
- ê¸°ì¡´ build_question_with_regression_context ì™€ ì´ë¦„ ì¶©ëŒ ë°©ì§€: build_regression_and_deixis_context ë¡œ ì œê³µ
- í•µì‹¬ ì•„ì´ë””ì–´:
  1) LLMìœ¼ë¡œ íšŒê·€ ì˜ë„ íŒì •(í‚¤ì›Œë“œ ê·œì¹™ X)
  2) íšŒê·€=Trueë©´ conversations.jsonì—ì„œ ê³¼ê±° ë§¥ë½ì„ ì‹¤ì œë¡œ 'ì„ íƒ'
  3) ì§ˆë¬¸ì— ì§€ì‹œì–´ê°€ ìˆìœ¼ë©´ íšŒê·€ ì—¬ë¶€ì™€ ë¬´ê´€í•˜ê²Œ JSONì—ì„œ ì‹œê°„/ì¥ì†Œ ì•µì»¤ë¥¼ ë³µì›í•´ [FACT]ë¡œ í”„ë¡¬í”„íŠ¸ ìƒë‹¨ì— ì£¼ì…
"""

from __future__ import annotations
from typing import Dict, Any, Tuple, List, Optional
import os, re, json
from datetime import datetime

from langchain_core.prompts import ChatPromptTemplate
from regress_conversation import _extract_meta, _llm_detect_regression, _db_load

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì™¸ë¶€ ì œê³µ/ê¸°ì¡´ í•¨ìˆ˜(ì´ë¯¸ í”„ë¡œì íŠ¸ì— ìˆëŠ” ê²ƒìœ¼ë¡œ ê°€ì •)
# - _db_load(): conversations.json ë¡œë“œ
# - _extract_meta(text): msg_keywords/kind/notes ë“± ë©”íƒ€ ì¶”ì¶œ(OpenAI ì‚¬ìš©)
# - _llm_detect_regression(question, summary_text, hist): íšŒê·€ ì—¬ë¶€/í‚¤ì›Œë“œ/ì´ìœ  ë“±
# - _to_text(): LangChain/OpenAI ì‘ë‹µì„ ë¬¸ìì—´ë¡œ ì •ê·œí™”
# â€» ì—†ìœ¼ë©´ ê¸°ì¡´ êµ¬í˜„ ì„í¬íŠ¸í•˜ì„¸ìš”.
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# from your_modules import _db_load, _extract_meta, _llm_detect_regression, _to_text

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Deixis(ì§€ì‹œì–´) í† í°: ì‹œê°„/ì¥ì†Œ/ì¸ë¬¼
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
DEIXIS_TIME_TOKENS = (
    "ì´ë•Œ", "ê·¸ë•Œ", "ê·¸ ë‚ ", "ê·¸ë‚ ", "ì´ë‚ ", "ê·¸ì¦ˆìŒ", "ê·¸ ë¬´ë µ", "ê·¸ ì‹œê¸°",
)
DEIXIS_PLACE_TOKENS = (
    "ê·¸ê³³", "ì´ê³³", "ê±°ê¸°", "ì €ê¸°", "ê·¸ ì¥ì†Œ", "ê·¸ ìœ„ì¹˜", "ê·¸ ì§€ì—­",
    "ê·¸ í˜¸í…”", "ê·¸ ë¦¬ì¡°íŠ¸", "ê·¸ ì¹´í˜", "ê·¸ ì‹ë‹¹", "ê·¸ ì—¬í–‰ì§€", "ê·¸ ë„ì‹œ", "ê·¸ ë‚˜ë¼",
)
DEIXIS_PERSON_TOKENS = (
    "ê·¸ ì‚¬ëŒ", "ì´ ì‚¬ëŒ", "ê·¸ë¶„", "ê·¸ ì—¬ì", "ê·¸ ë‚¨ì", "ê·¸ ì¹œêµ¬", "ê·¸ ì• ",
)

def _has_deixis(q: str) -> bool:
    """ì§ˆë¬¸ì— ì‹œê°„/ì¥ì†Œ/ì¸ë¬¼ ì§€ì‹œì–´ê°€ í•˜ë‚˜ë¼ë„ ìˆìœ¼ë©´ True"""
    if not q: return False
    qs = " ".join(str(q).split())
    toks = DEIXIS_TIME_TOKENS + DEIXIS_PLACE_TOKENS + DEIXIS_PERSON_TOKENS
    return any(tok in qs for tok in toks)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì„¸ì…˜ íˆìŠ¤í† ë¦¬ ìœ ë¬´/ê¸¸ì´
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _get_history_stats(*, session_id: str) -> dict:
    """
    í˜„ì¬ ì„¸ì…˜ì˜ ê³¼ê±° í„´ ìˆ˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ 'íˆìŠ¤í† ë¦¬ ì¡´ì¬ ì—¬ë¶€' íŒë‹¨.
    - ë°˜ë“œì‹œ session_idë¥¼ ë°›ì•„ì„œ sid=None ë¬¸ì œë¥¼ ì›ì²œ ì°¨ë‹¨.
    """
    db = _db_load()
    sess = (db.get("sessions") or {}).get(session_id) or {}
    turns = (sess.get("turns") or [])
    return {"has_history": len(turns) > 0, "history_turns": len(turns)}

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ê³¼ê±° ë§¥ë½ ì„ íƒ: í‚¤ì›Œë“œ Jaccard + kind ë³´ë„ˆìŠ¤
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _jaccard(a: set[str], b: set[str]) -> float:
    if not a and not b: return 0.0
    inter = len(a & b)
    union = len(a | b)
    return inter / union if union else 0.0

def _select_context_from_json(
    *, merged_kws: List[str], target_kind: Optional[str], limit_pick: int, session_id: str
) -> Tuple[List[dict], dict]:
    """
    conversations.json â†’ sessions[session_id].turnsì—ì„œ ìµœê·¼â†’ê³¼ê±°ë¡œ ìŠ¤ìº”,
    í‚¤ì›Œë“œ ê²¹ì¹¨/Jaccard + kind ì¼ì¹˜ ë³´ë„ˆìŠ¤ë¡œ ìŠ¤ì½”ì–´ë§í•˜ì—¬ ìƒìœ„ Nê°œ í”½.
    ë°˜í™˜: (LLM í”„ë¡¬í”„íŠ¸ìš© í¬ë§· ë¦¬ìŠ¤íŠ¸, ë””ë²„ê·¸)
    """
    db = _db_load()
    sess = (db.get("sessions") or {}).get(session_id) or {}
    turns = (sess.get("turns") or [])[:]
    total = len(turns)
    turns.reverse()  # ìµœì‹ â†’ê³¼ê±°

    now_kws = set([k.strip().lower() for k in (merged_kws or []) if k])
    scored: List[Tuple[float, dict]] = []

    for t in turns:
        prev_kws = set([k.strip().lower() for k in (t.get("msg_keywords") or []) if k])
        s = _jaccard(now_kws, prev_kws)
        if target_kind and (t.get("kind") or "").strip().lower() == target_kind:
            s += 0.15
        if s > 0:
            scored.append((s, t))

    scored.sort(key=lambda x: x[0], reverse=True)
    picked = [t for _, t in scored[:limit_pick]]

    # í¬ë§·(LLMì— ë³´ì—¬ì¤„ ë‹¨ë¬¸ ë¼ì¸)
    rows_fmt = [
        {
            "date": t.get("date",""),
            "time": t.get("time",""),
            "role": t.get("role",""),
            "text": (t.get("text") or "").strip().replace("\n"," "),
        } for t in picked
    ]
    dbg = {
        "searched_total": total,
        "now_keywords": list(now_kws),
        "now_kind": target_kind,
        "scored": len(scored),
        "filtered_by_min_sim": len(scored),  # (ê°„ë‹¨í™”)
        "picked": len(picked)
    }
    return rows_fmt, dbg

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì ˆëŒ€ë‚ ì§œ(YYYYë…„ Mì›” Dì¼) íŒŒì‹±
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
_DATE_KR_RE = re.compile(r"(\d{4})\s*ë…„\s*(\d{1,2})\s*ì›”\s*(\d{1,2})\s*ì¼")

def _parse_abs_kr_date(text: str) -> Optional[str]:
    """í…ìŠ¤íŠ¸ì—ì„œ 'YYYYë…„ Mì›” Dì¼' â†’ 'YYYY-MM-DD'"""
    if not text: return None
    m = _DATE_KR_RE.search(text)
    if not m: return None
    y, mth, d = map(int, m.groups())
    try:
        _ = datetime(y, mth, d)  # ìœ íš¨ì„± ê²€ì‚¬
        return f"{y:04d}-{mth:02d}-{d:02d}"
    except ValueError:
        return None

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì‹œê°„ ì•µì»¤(ë‚ ì§œ) ë³µì›
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _find_temporal_anchor_from_json(
    session_id: str, *, topic_hints: Tuple[str,...] = ("ì—¬í–‰","ë§Œë‚¨")
) -> Tuple[Optional[str], dict]:
    """
    ìµœì‹ â†’ê³¼ê±°ë¡œ ìŠ¤ìº”í•˜ë©° ì‹œê°„ ì•µì»¤ë¥¼ ë³µì›:
      1) assistant í…ìŠ¤íŠ¸ì— ì ˆëŒ€ë‚ ì§œ + ì£¼ì œ íŒíŠ¸ â†’ ìµœê³  ì‹ ë¢°
      2) turn.target_date í•„ë“œ (user/assistant)
      3) ì ˆëŒ€ë‚ ì§œë§Œ ìˆëŠ” í…ìŠ¤íŠ¸
    """
    db = _db_load()
    sess = (db.get("sessions") or {}).get(session_id) or {}
    turns = (sess.get("turns") or [])[:]
    turns.reverse()

    print(f"[DEIXIS][TIME] scan sid={session_id} n={len(turns)}")
    searched = 0

    for t in turns:
        searched += 1
        role = t.get("role","")
        txt  = (t.get("text") or "")

        d1 = _parse_abs_kr_date(txt)
        if d1 and any(h in txt for h in topic_hints):
            return d1, {"source":"assistant_text" if role=="assistant" else "text_with_hint", "searched":searched}

        td = t.get("target_date")
        if td and any(h in (txt or "") for h in topic_hints + ("ì—¬í–‰ìš´","ì¼ì •","ë‚ ì§œ")):
            return td, {"source":"turn.target_date", "searched":searched}

        if d1:
            return d1, {"source":"text_abs_date", "searched":searched}

    return None, {"source":"none", "searched":searched}

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì¥ì†Œ ì•µì»¤(íœ´ë¦¬ìŠ¤í‹±) ë³µì›
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
_PLACE_AFTER_HEAD_RE = re.compile(
    r"(?:ì—¬í–‰ì§€|ì¥ì†Œ|ìœ„ì¹˜|ë„ì‹œ|êµ­ê°€|í˜¸í…”|ë¦¬ì¡°íŠ¸|ê³µì›|í•´ë³€|ì¹´í˜|ì‹ë‹¹)\s*(?:ì€|ëŠ”|ì´|ê°€|ìœ¼ë¡œ|ë¡œ|ì—ì„œ|ì—)?\s*([ê°€-í£A-Za-z0-9Â·\- ]{2,30})"
)
_PLACE_BEFORE_JOSA_RE = re.compile(
    r"([ê°€-í£A-Za-z0-9Â·\- ]{2,30})(?:ì—ì„œ|ìœ¼ë¡œ|ë¡œ|ì—)\s*(?:ë§Œë‚¬|ì—¬í–‰|ì¶œë°œ|ê°„|ì™”ë‹¤|ë¨¸ë¬¸|ë¬µì—ˆ|ë´¤|ì˜ˆì•½|ì°ì—ˆ)"
)

def _extract_place_candidate(text: str) -> Optional[str]:
    """ë¬¸ì¥ì—ì„œ ì¥ì†Œ ë‹¨ì„œë¥¼ ê°€ë³ê²Œ ì¶”ì¶œ(íœ´ë¦¬ìŠ¤í‹±)"""
    if not text: return None
    m = _PLACE_AFTER_HEAD_RE.search(text)
    if m: return m.group(1).strip()
    m = _PLACE_BEFORE_JOSA_RE.search(text)
    if m: return m.group(1).strip()
    return None

def _find_place_anchor_from_json(
    session_id: str, *, topic_hints: Tuple[str,...] = ("ì—¬í–‰","ë§Œë‚¨","ì¥ì†Œ","í˜¸í…”","ì¹´í˜","ë„ì‹œ","êµ­ê°€")
) -> Tuple[Optional[str], dict]:
    """
    ìµœì‹ â†’ê³¼ê±°ë¡œ ìŠ¤ìº”í•˜ë©° ì¥ì†Œ ë‹¨ì„œë¥¼ ë³µì›.
    - êµ¬ì¡°í™” í•„ë“œê°€ ì—†ë‹¤ëŠ” ì „ì œì—ì„œ í…ìŠ¤íŠ¸ íœ´ë¦¬ìŠ¤í‹±ë§Œ ì‚¬ìš©(ê°€ë²¼ì›€)
    - topic_hints ê°€ í¬í•¨ëœ ë¬¸ì¥ì„ ìš°ì„  ì±„íƒ
    """
    db = _db_load()
    sess = (db.get("sessions") or {}).get(session_id) or {}
    turns = (sess.get("turns") or [])[:]
    turns.reverse()

    print(f"[DEIXIS][PLACE] scan sid={session_id} n={len(turns)}")
    searched = 0
    fallback = None

    for t in turns:
        searched += 1
        txt = (t.get("text") or "")
        cand = _extract_place_candidate(txt)
        if not cand:
            continue
        if any(h in txt for h in topic_hints):
            return cand, {"source":"text_place_with_hint", "searched":searched}
        if not fallback:
            fallback = cand

    if fallback:
        return fallback, {"source":"text_place_fallback", "searched":searched}
    return None, {"source":"none", "searched":searched}

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Step A: LLM íšŒê·€ íŒì • (ì˜ë¯¸ ê¸°ë°˜, ë£°/ë§ˆì»¤ ì œê±°)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

_CONTINUATION_DETECT_PROMPT = ChatPromptTemplate.from_messages([
    ("system", """ë„ˆëŠ” "ëŒ€í™” íšŒê·€ ì—¬ë¶€ íŒì •ê¸°"ë‹¤.
ì•„ë˜ì˜ "ì§ì „ assistant ë‹µë³€"ê³¼ "í˜„ì¬ user ì§ˆë¬¸"ì„ ë³´ê³ ,
í˜„ì¬ ì§ˆë¬¸ì´ ì§ì „ ë‹µë³€ì„ ì „ì œë¡œ ì˜ë¯¸ì ìœ¼ë¡œ ì´ì–´ì§€ëŠ” ì§ˆë¬¸ì¸ì§€ íŒì •í•˜ë¼.

ê·œì¹™:
- ì¶”ì¸¡ ê¸ˆì§€. í…ìŠ¤íŠ¸ ê·¼ê±°ê°€ ì•½í•˜ë©´ False.
- "ì´ì „ ë‹µë³€ì„ ì „ì œ(ê·¸ ë‹µë³€ì˜ ê²°ë¡ /ë‚´ìš©/ì„ íƒì§€/ì„¤ëª…ì„ ë°”íƒ•ìœ¼ë¡œ)" í•˜ë©´ True.
- ì™„ì „íˆ ìƒˆ ì£¼ì œë©´ False.
- ì¶œë ¥ì€ JSONë§Œ. ë‹¤ë¥¸ í…ìŠ¤íŠ¸ ê¸ˆì§€.

ì¶œë ¥ ìŠ¤í‚¤ë§ˆ:
{{
  "is_continuation": true/false,
  "confidence": 0.0-1.0,
  "reason": "í•œ ë¬¸ì¥"
}}
"""),
    ("user", """ì§ì „ assistant ë‹µë³€:
<<<
{prev_assistant_text}
>>>

í˜„ì¬ user ì§ˆë¬¸:
<<<
{current_question}
>>>

JSONë§Œ ì¶œë ¥.""")
])


def _llm_detect_continuation_v2(question: str, prev_assistant_text: str) -> dict:
    """
    LLMìœ¼ë¡œ íšŒê·€ ì—¬ë¶€ íŒì • (ì˜ë¯¸ ê¸°ë°˜, ë§ˆì»¤/ë£° ì—†ìŒ)
    
    Args:
        question: í˜„ì¬ user ì§ˆë¬¸
        prev_assistant_text: ì§ì „ assistant ë‹µë³€ (ìµœê·¼ 300ì)
    
    Returns:
        {
            "is_continuation": bool,
            "confidence": float,
            "reason": str
        }
    """
    try:
        import os
        from langchain_openai import ChatOpenAI
        
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            print("[REG][STEP-A] OPENAI_API_KEY not set")
            return {
                "is_continuation": False,
                "confidence": 0.0,
                "reason": "api_key_missing"
            }
        
        llm = ChatOpenAI(
            model="gpt-4o-mini",
            temperature=0.0,
            max_tokens=200,
            timeout=15,
            openai_api_key=api_key,
            model_kwargs={"response_format": {"type": "json_object"}}
        )
        
        chain = _CONTINUATION_DETECT_PROMPT | llm
        result = chain.invoke({
            "prev_assistant_text": prev_assistant_text[:300],  # ìµœê·¼ 300ìë§Œ
            "current_question": question
        })
        
        import json
        data = json.loads(result.content if hasattr(result, "content") else str(result))
        
        # ê¸°ë³¸ê°’ ë³´ì •
        data.setdefault("is_continuation", False)
        data.setdefault("confidence", 0.0)
        data.setdefault("reason", "")
        
        return data
        
    except Exception as e:
        print(f"[REG][STEP-A] LLM íŒì • ì‹¤íŒ¨: {e}")
        return {
            "is_continuation": False,
            "confidence": 0.0,
            "reason": f"exception: {e}"
        }



# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Step B: ì´ì „ ê²°ë¡  LLM ì •ì œ (íšŒê·€ì¼ ë•Œë§Œ, í† í”½ í‚¤ì›Œë“œ ì—†ì´)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

_REFINE_CONCLUSIONS_PROMPT = ChatPromptTemplate.from_messages([
    ("system", """ë„ˆëŠ” ëŒ€í™” ìš”ì•½ê¸°ë‹¤. ìƒˆ íŒë‹¨ì„ ë§Œë“¤ì§€ ë§ê³ , ì´ë¯¸ ë‚˜ì˜¨ íŒë‹¨ë§Œ ì •ì œ/ìš”ì•½í•œë‹¤.

ğŸš¨ í•µì‹¬ ê·œì¹™ (ì ˆëŒ€ ì¤€ìˆ˜):
1. **í…ìŠ¤íŠ¸ì— ì—†ëŠ” ë‚´ìš©ì€ ë§Œë“¤ì§€ ë§ˆë¼.**
2. **ê°€ëŠ¥í•˜ë©´ ë¹ˆ ë°°ì—´ë¡œ ë‘ì–´ë¼.** (ë¶ˆí™•ì‹¤í•˜ë©´ ë¹„ìš°ê¸°)
3. ì¶”ì •/í•´ì„ ê¸ˆì§€. ëª…ì‹œì ìœ¼ë¡œ ë‚˜ì˜¨ ê²°ë¡ ë§Œ ì¶”ì¶œ.

ì¶œë ¥ JSON:
{{
  "decisions": ["ì´ì „ì— ë„ì¶œëœ í•µì‹¬ ê²°ë¡  (ëª…í™•í•œ ê²ƒë§Œ)"],
  "key_points": ["ì¤‘ìš” íŒë‹¨ ìš”ì§€"],
  "open_questions": ["ì•„ì§ ë‹µ ì•ˆ ëœ í•µì‹¬ ì§ˆë¬¸"],
  "constraints": ["ë³´ìˆ˜ì  ì ‘ê·¼", "ë¦¬ìŠ¤í¬ íšŒí”¼" ë“± ì¡°ê±´/ì œì•½],
  "confidence": 0.0~1.0
}}

confidence ê°€ì´ë“œ:
- ëª…í™•í•œ ê²°ë¡ ì´ ì—¬ëŸ¬ ê°œ â†’ 0.8~1.0
- ì¼ë¶€ ê²°ë¡ ë§Œ ëª…í™• â†’ 0.5~0.7
- ì• ë§¤í•˜ê±°ë‚˜ ì¶”ì • í•„ìš” â†’ 0.3 ì´í•˜
- ê²°ë¡  ì—†ìŒ â†’ 0.0 (ë¹ˆ ë°°ì—´)
"""),
    ("user", """ìµœê·¼ ë‹µë³€ë“¤:
{assistant_messages}

í˜„ì¬ ì§ˆë¬¸:
{current_question}

JSONë§Œ ì¶œë ¥.""")
])

def _refine_conclusions_with_llm(rows_fmt: List[dict], current_question: str) -> dict:
    """
    LLMìœ¼ë¡œ ì´ì „ ê²°ë¡  ì •ì œ (í† í”½ í‚¤ì›Œë“œ ì—†ì´ ì˜ë¯¸ ê¸°ë°˜)
    
    Returns:
        {
            "decisions": [...],
            "key_points": [...],
            "open_questions": [...],
            "constraints": [...],
            "confidence": 0.0~1.0
        }
    """
    if not rows_fmt:
        return {"decisions": [], "key_points": [], "open_questions": [], "constraints": [], "confidence": 0.0}
    
    # ìµœê·¼ assistant ë‹µë³€ 2~4ê°œë§Œ (ë¹„ìš© ìµœì†Œí™”)
    assistant_msgs = [r for r in rows_fmt if r.get("role") == "assistant"][-4:]
    if not assistant_msgs:
        return {"decisions": [], "key_points": [], "open_questions": [], "constraints": [], "confidence": 0.0}
    
    assistant_text = "\n\n".join([f"[ë‹µë³€{i+1}] {m.get('text', '')[:200]}" for i, m in enumerate(assistant_msgs)])
    
    try:
        # LLM í˜¸ì¶œ
        import os
        from langchain_openai import ChatOpenAI
        
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            print("[REFINE] OPENAI_API_KEY not set")
            return {"decisions": [], "key_points": [], "open_questions": [], "constraints": [], "confidence": 0.0}
        
        llm = ChatOpenAI(
            model="gpt-4o-mini",
            temperature=0.0,
            max_tokens=400,
            timeout=15,
            openai_api_key=api_key,
            model_kwargs={"response_format": {"type": "json_object"}}
        )
        
        chain = _REFINE_CONCLUSIONS_PROMPT | llm
        result = chain.invoke({
            "assistant_messages": assistant_text,
            "current_question": current_question
        })
        
        import json
        data = json.loads(result.content if hasattr(result, "content") else str(result))
        
        # ê¸°ë³¸ê°’ ë³´ì •
        data.setdefault("decisions", [])
        data.setdefault("key_points", [])
        data.setdefault("open_questions", [])
        data.setdefault("constraints", [])
        data.setdefault("confidence", 0.0)
        
        return data
        
    except Exception as e:
        print(f"[REFINE] LLM ì •ì œ ì‹¤íŒ¨: {e}")
        return {"decisions": [], "key_points": [], "open_questions": [], "constraints": [], "confidence": 0.0}


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì§€ì‹œì–´ í•´ì„ â†’ FACT ìƒì„±
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _resolve_deixis_and_make_facts(question: str, *, session_id: str, meta_now: dict) -> dict:
    """
    ì§ˆë¬¸ì— ì§€ì‹œì–´ê°€ ìˆìœ¼ë©´:
      - ì‹œê°„ ì•µì»¤(ë‚ ì§œ)
      - ì¥ì†Œ ì•µì»¤(ì¥ì†Œëª…)
      - ì¸ë¬¼ ì§€ì‹œ ê³ ì •("ì´ë•Œ ë§Œë‚œ ì‚¬ëŒ" â†’ í•´ë‹¹ ì‹œì /ì¥ì†Œì˜ ë§Œë‚¨)
    ì„ FACTë¡œ êµ¬ì„±í•´ ë°˜í™˜.
    """
    facts: dict = {}
    if not _has_deixis(question):
        return facts

    hints = tuple(meta_now.get("msg_keywords") or []) + ("ì—¬í–‰","ë§Œë‚¨","ì¼ì •","ì¥ì†Œ","í˜¸í…”","ë„ì‹œ")

    # ì‹œê°„
    anchor_date, tdbg = _find_temporal_anchor_from_json(session_id, topic_hints=hints)
    if anchor_date:
        facts["deixis_anchor_date"] = {"value": anchor_date, "source": tdbg.get("source")}

    # ì¥ì†Œ
    anchor_place, pdbg = _find_place_anchor_from_json(session_id, topic_hints=hints)
    if anchor_place:
        facts["deixis_anchor_place"] = {"value": anchor_place, "source": pdbg.get("source")}

    # ì¸ë¬¼(ì‚¬ëŒ ì´ë¦„ì´ ì—†ìœ¼ë¯€ë¡œ "ê·¸ ì‹œì /ì¥ì†Œì˜ ë§Œë‚¨"ìœ¼ë¡œ ê³ ì •)
    q = question
    if any(tok in q for tok in DEIXIS_PERSON_TOKENS) or "ë§Œë‚œ" in q or "ë§Œë‚¨" in q:
        val = "í•´ë‹¹ ì‹œì ì˜ ë§Œë‚¨(ìµœê·¼ ëŒ€í™”)"
        if anchor_date and anchor_place:
            val = f"{anchor_date} {anchor_place}ì—ì„œì˜ ë§Œë‚¨(ìµœê·¼ ëŒ€í™”)"
        elif anchor_date:
            val = f"{anchor_date}ì˜ ë§Œë‚¨(ìµœê·¼ ëŒ€í™”)"
        elif anchor_place:
            val = f"{anchor_place}ì—ì„œì˜ ë§Œë‚¨(ìµœê·¼ ëŒ€í™”)"
        facts["deixis_person"] = {"value": val, "source": "inferred_from_anchor"}
        
    print(f"[DEIXIS] facts={facts}")        #
    return facts

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë©”ì¸ ë¹Œë”(V2): íšŒê·€ + ì§€ì‹œì–´ FACT í†µí•©
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
from typing import Dict, Any, Tuple, List

def build_regression_and_deixis_context(
    question: str,
    summary_text: str,
    *,
    session_id: str,
) -> Tuple[str, dict]:
    """
    ğŸ¯ í•˜ì´ë¸Œë¦¬ë“œ íšŒê·€ ì²˜ë¦¬ (Rule + LLM ì •ì œ)
    
    Pipeline:
      Step A: Rule-based íšŒê·€ íŒì • (ëŒ€í™” êµ¬ì¡°ë§Œ)
      Step B: LLM ì´ì „ ê²°ë¡  ì •ì œ (ì˜ë¯¸ ê¸°ë°˜, í† í”½ í‚¤ì›Œë“œ âŒ)
      Step C: ì¡°ê±´ë¶€ memory_summary ì£¼ì… (confidence â‰¥ ì„ê³„ì¹˜)
    
    Returns:
        (í”„ë¡¬í”„íŠ¸, ë””ë²„ê·¸ë©”íƒ€)
    """
    def _brief(s: str, n: int = 140) -> str:
        if not s:
            return ""
        s = str(s).replace("\n", " ").strip()
        return (s[:n] + "â€¦") if len(s) > n else s
    
    print(f"[REG][IN] session_id={session_id}")
    print(f"[REG][IN] question='{_brief(question)}'")
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # 1) íˆìŠ¤í† ë¦¬ ê²Œì´íŠ¸ (ì²« í„´ì´ë©´ íšŒê·€ ë¶ˆê°€)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    hist = _get_history_stats(session_id=session_id)
    if not hist.get("has_history"):
        dbg = {"step": "A", "is_continuation": False, "confidence": 0.0, "reason": "first_turn"}
        return question, dbg
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Step A: LLM íšŒê·€ íŒì • (ì˜ë¯¸ ê¸°ë°˜)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # ì§ì „ assistant ë‹µë³€ ê°€ì ¸ì˜¤ê¸°
    from regress_conversation import _db_load
    db = _db_load()
    sessions = db.get("sessions") or {}
    sess = sessions.get(session_id) or {}
    turns = list(sess.get("turns") or [])
    
    prev_assistant_text = ""
    for t in reversed(turns):
        if t.get("role") == "assistant":
            prev_assistant_text = t.get("text", "")[:500]  # ìµœê·¼ 500ì
            break
    
    continuation_result = _llm_detect_continuation_v2(question, prev_assistant_text)
    print(f"[REG][STEP-A] is_continuation={continuation_result['is_continuation']} "
          f"confidence={continuation_result['confidence']:.2f} "
          f"reason='{continuation_result['reason']}'")
    
    CONTINUATION_THRESHOLD = 0.75  # âœ… ë³´ìˆ˜ì  ì„ê³„ì¹˜
    
    # íšŒê·€ ì•„ë‹ˆë©´ ì¦‰ì‹œ ì¢…ë£Œ
    if not continuation_result["is_continuation"] or continuation_result["confidence"] < CONTINUATION_THRESHOLD:
        dbg = {
            "step": "A",
            "is_continuation": continuation_result["is_continuation"],
            "confidence": continuation_result["confidence"],
            "reason": continuation_result["reason"],
            "below_threshold": continuation_result["confidence"] < CONTINUATION_THRESHOLD
        }
        return question, dbg
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Step B: LLM ì´ì „ ê²°ë¡  ì •ì œ (íšŒê·€ì¼ ë•Œë§Œ í˜¸ì¶œ)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # ìµœê·¼ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
    from regress_conversation import _extract_meta
    meta_now = _extract_meta(question)
    
    # ê³¼ê±° ë§¥ë½ ê²€ìƒ‰ (í‚¤ì›Œë“œ ê¸°ë°˜, ìƒìœ„ 4ê°œë§Œ)
    merged_kws = meta_now.get("msg_keywords", [])
    try:
        rows_fmt, scan_dbg = _select_context_from_json(
            merged_kws=merged_kws,
            target_kind=meta_now.get("kind"),
            limit_pick=4,  # ë¹„ìš© ìµœì†Œí™”
            session_id=session_id
        )
    except Exception as e:
        print(f"[REG][STEP-B] context scan failed: {e}")
        rows_fmt, scan_dbg = [], {}
    
    # LLM ì •ì œ
    refined = _refine_conclusions_with_llm(rows_fmt, question)
    decisions_count = len(refined.get("decisions", []))
    print(f"[REG][STEP-B] refined_confidence={refined['confidence']:.2f} decisions_count={decisions_count}")
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Step C: ì¡°ê±´ë¶€ ì£¼ì… (ë³´ìˆ˜ì )
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    CONFIDENCE_THRESHOLD = 0.6
    
    if refined["confidence"] < CONFIDENCE_THRESHOLD:
        print(f"[REG][STEP-C] injected=False reason=low_confidence ({refined['confidence']:.2f} < {CONFIDENCE_THRESHOLD})")
        dbg = {
            "step": "C_skipped",
            "is_continuation": True,
            "llm_confidence": continuation_result["confidence"],
            "llm_reason": continuation_result["reason"],
            "refined_confidence": refined["confidence"],
            "decisions_count": decisions_count,
            "injected": False,
            "reason": "low_confidence"
        }
        return question, dbg
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # í”„ë¡¬í”„íŠ¸ êµ¬ì„± (ìµœì†Œí™”, ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì²´)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    header = []
    header.append(f"ì‚¬ìš©ìê°€ ì´ì „ ëŒ€í™”ë¥¼ ì´ì–´ì„œ ì§ˆë¬¸í•˜ê³  ìˆìŠµë‹ˆë‹¤. (íšŒê·€ ì‹ ë¢°ë„={continuation_result['confidence']:.2f})")
    
    # memory_summary (1~3ì¤„ ì œí•œ, íƒœê·¸ í˜•ì‹ í”¼í•˜ê¸°)
    context_lines = []
    if refined.get("decisions"):
        decisions_text = ", ".join(refined['decisions'][:2])
        context_lines.append(f"ì´ì „ ëŒ€í™” ìš”ì•½: {decisions_text}")
    
    if refined.get("open_questions") and len(context_lines) < 3:
        context_lines.append(f"í˜„ì¬ ì§ˆë¬¸ì€ ì´ì „ ê²°ë¡ ì„ ì „ì œë¡œ í•¨: {refined['open_questions'][0]}")
    
    if refined.get("constraints") and len(context_lines) < 3:
        constraints_text = ", ".join(refined['constraints'][:2])
        context_lines.append(f"ì°¸ê³ : {constraints_text}")
    
    # âœ… ìµœëŒ€ 3ì¤„ê¹Œì§€ë§Œ ì£¼ì…
    if context_lines:
        header.extend(context_lines[:3])
    
    # ì§€ì‹œì–´ FACT (ìˆìœ¼ë©´)
    try:
        facts = _resolve_deixis_and_make_facts(question, session_id=session_id, meta_now=meta_now)
    except Exception as e:
        print(f"[REG][DEIXIS] failed: {e}")
        facts = {}
    
    if "deixis_anchor_date" in facts:
        header.append(f"[FACT] 'ì´ë•Œ'ëŠ” {facts['deixis_anchor_date']['value']}")
    
    # ê³¼ê±° ëŒ€í™” ë¼ì¸ì—… (ê°„ëµ, ìµœëŒ€ 3ê°œ)
    lines = [f"- {r.get('role','')}: {r.get('text','')[:80]}..." for r in rows_fmt[:3]]
    
    body = "\n".join(header)
    if lines:
        body += f"\n\nê³¼ê±° ëŒ€í™” ìš”ì•½:\n" + "\n".join(lines)
    
    prompt = f"{body}\n\ní˜„ì¬ ì§ˆë¬¸: {question}"
    
    print(f"[REG][STEP-C] injected=True prompt_length={len(prompt)} chars")
    
    dbg = {
        "step": "C_injected",
        "is_continuation": True,
        "llm_confidence": continuation_result["confidence"],
        "llm_reason": continuation_result["reason"],
        "refined_confidence": refined["confidence"],
        "decisions_count": decisions_count,
        "injected": True,
        "reason": "confidence_ok",
        "refined": refined,
        "facts": facts
    }
    
    return prompt, dbg


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë¸Œë¦¿ì§€ í…ìŠ¤íŠ¸ ìƒì„± (ê¸°ì¡´ í˜¸í™˜)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _make_bridge(facts: dict | None) -> str:
    """íšŒê·€ ì‹œ ë‚´ë¶€ ì°¸ê³  ë©”ëª¨. ë‹µë³€ì— ë…¸ì¶œ ê¸ˆì§€ ê°€ì •."""
    facts = facts or {}
    bits = []

    d = (facts.get("deixis_anchor_date") or {}).get("value")
    if d:
        bits.append(f"ì‚¬ìš©ì ì§ˆë¬¸ì˜ 'ê·¸ë‚ /ì´ë•Œ'ëŠ” {d}ë¥¼ ê°€ë¦¬í‚´")

    trip = (facts.get("trip_date") or {}).get("value")
    if trip and trip != d:
        bits.append(f"ìµœê·¼ íšŒìˆ˜ëœ ì—¬í–‰ ë‚ ì§œëŠ” {trip}")

    # ì‚¬ëŒ/ì¥ì†Œ ê°™ì€ ì¶”ê°€ íŒ©íŠ¸ê°€ ìˆìœ¼ë©´ ê°™ì€ ë°©ì‹ìœ¼ë¡œ ë¶™ì´ì„¸ìš”.
    # place = (facts.get("place") or {}).get("value")
    # if place: bits.append(f"ì—¬í–‰ ì¥ì†Œ: {place}")

    return " / ".join(bits)  # â† 'ì´ì–´ì„œ...' ê°™ì€ ì„œë‘ ì—†ìŒ

    
    
    
# ChatPromptTemplateëŠ” ì´ë¯¸ ìƒë‹¨ì—ì„œ importë¨

counseling_prompt = ChatPromptTemplate.from_messages([
    ("system", """ë„ˆëŠ” ë§¥ë½ì„ ì •í™•íˆ ì´ì–´ì£¼ëŠ” í•œêµ­ì–´ ì‚¬ì£¼ ìƒë‹´ê°€ë‹¤.

ì¶œë ¥ ì›ì¹™(ë§¤ìš° ì¤‘ìš”):
- ë°˜ë“œì‹œ ì²« ë¬¸ì¥ì€ ê·¸ëŒ€ë¡œ ì¶œë ¥í•œë‹¤: "{bridge}"
- ì•„ë˜ [FACTS]ì˜ ì •ë³´ê°€ ìˆìœ¼ë©´, ì²« 1~2ë¬¸ì¥ì— ìì—°ìŠ¤ëŸ½ê²Œ ëª…ì‹œí•˜ë¼(ë‚ ì§œ/ì¥ì†Œ/ê·¸ ì‚¬ëŒ ë“±).
- [CONTEXT]ì˜ ê³¼ê±° ëŒ€í™”ì™€ í˜„ì¬ ì§ˆë¬¸ì„ ì—°ê²°í•´ 'ë§¥ë½ ë¸Œë¦¿ì§€'ë¥¼ ë§Œë“  ë’¤, ê·¸ ë§¥ë½ì—ì„œë§Œ í•´ì„í•˜ë¼.
- ì§ˆë¬¸ ë²”ìœ„ ë°–ì˜ ì£¼ì œ(ì˜ˆ: 'ê²°í˜¼' ë“±)ë¡œ í™•ì¥í•˜ì§€ ë§ˆë¼. ê³¼ì¥/ì˜ˆì–¸/ë‹¨ì • ì–´íˆ¬ ê¸ˆì§€.
- ë¬¸ì²´: ë”°ëœ»í•˜ê³  ì°¨ë¶„, 4~7ë¬¸ì¥. ë§ˆì§€ë§‰ì— 'ğŸ” í¬ì¸íŠ¸:' í•œ ì¤„ ìš”ì•½.

ê¸ˆì§€:
- ê·¼ê±° ì—†ì´ ë‹¤ë¥¸ ì£¼ì œ(ê²°í˜¼, ìŠ¹ì§„ ë“±)ë¡œ ë¹„ì•½í•˜ê¸°.
- [CONTEXT]ì— ì—†ëŠ” ì‚¬ì‹¤ì„ ë‹¨ì •í•˜ê¸°.
- ì¤‘ë³µëœ ì¼ë°˜ë¡  ë‚˜ì—´.

"""),
    ("user", """
[CONTEXT]
{context}

[FACTS]
{facts}

[ìš”ì•½]
{summary}

[ì‚¬ìš©ì ì§ˆë¬¸]
{user_question}
""")
])
